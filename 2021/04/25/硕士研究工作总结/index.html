

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/bg/xiao.png">
  <link rel="icon" href="/img/bg/xiao.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  
    <meta name="description" content="">
  
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
  <title>LihengXu&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"zTPGNcnnYmT218Av5iBL8nGh-gzGzoHsz","app_key":"Nk68rUDzBxauuv3b7ro1QdtD","server_url":"https://ztpgncnn.lc-cn-n1-shared.com"}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LihengXu</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                Links
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                about
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg/example.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-04-25 23:05" pubdate>
        April 25, 2021 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      16.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      228
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none"></h1>
            
            <div class="markdown-body">
              <h1 id="2019-2022研究工作总结"><a href="#2019-2022研究工作总结" class="headerlink" title="2019-2022研究工作总结"></a>2019-2022研究工作总结</h1><p>本文主要是对于我整个研究生期间开展研究工作的阶段性总结，我的所有工作都离不开我的导师和博士师兄的指导，也离不开小组内师兄师姐和同门的帮助，非常感谢他们为我做出的种种付出。</p>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>我们小组的工作主要关注于无人驾驶智能感知系统的测试。最开始研一刚入学，我师兄给我提出了一个问题：通常无人驾驶环境感知系统总是存在着一些特定的缺陷，具体表现可能是在某种特定的边缘场景（最初的定义）下性能表现会急剧下降，甚至失效，我们是否能够有效的挖掘出这些边缘场景，用这些边缘场景进行无人驾驶场景测试或者用来针对性学习这些边缘场景的特性以提高智能感知算法的鲁棒性。这个问题的实际解决其实是非常具有实际意义和价值的。</p>
<p>基于这个最初的想法，我们最后不断的讨论和修正，定义为了一个针对无人驾驶感知算法的最差感知场景搜索问题。这个问题也是我所有工作的主线，下面我会主要围绕着这个主线进行总结。在整个工作是进展过程中，也涉及到了一些和该工作相关的一些其他工作的实验补充，我也会按一条完整的逻辑线整理一遍。</p>
<p>本文的主要结构：</p>
<ul>
<li>无人驾驶最差感知场景搜索<strong>问题的提出</strong>（第一部分）</li>
<li>无人驾驶最差感知场景搜索问题的<strong>背景</strong>（第二部分）</li>
<li>无人驾驶最差感知<strong>场景的表征方式</strong>（第三部分）</li>
<li>无人驾驶最差感知场景搜素<strong>框架设计</strong>（第四部分）</li>
<li>无人驾驶最差感知场景搜素框架<strong>有效性验证</strong>（第五部分）</li>
<li>无人驾驶最差感知场景搜索实验的<strong>发现和扩展</strong>（第六部分）</li>
<li>无人驾驶最差感知场景搜索框架的<strong>具体应用</strong>（第七部分）</li>
</ul>
<p>对应的主要成果：</p>
<ul>
<li>第1-6部分内容<ul>
<li>Worst Perception Scenario Search for Autonomous Driving.   IV2020（核心内容，一作）</li>
<li>Worst Perception Scenario Search via Recurrent Neural Controller and K-reciprocal Re-ranking.     T- CSVT（核心内容，共一）</li>
</ul>
</li>
<li>第7部分内容<ul>
<li>Level-aware Haze Synthesis by self-supervised Content-Style Disentanglement.   T- CSVT（部分补充实验，三作）</li>
</ul>
</li>
</ul>
<h2 id="2-无人驾驶最差感知场景搜索问题的提出"><a href="#2-无人驾驶最差感知场景搜索问题的提出" class="headerlink" title="2. 无人驾驶最差感知场景搜索问题的提出"></a>2. 无人驾驶最差感知场景搜索问题的提出</h2><p>测试和验证方法的进步是自动驾驶商业化和标准化的关键要求之一。随着自动驾驶等级的提高，汽车系统和涉及自动驾驶的算法复杂性进一步增加，多变的天气、复杂的交通环境、多样的驾驶任务和动态的行驶状态，都为自动驾驶系统测试评估提出了新的挑战，由此导致操作设计域ODD不断增大以及OTA特征不断更新，一些传统测试方法，例如封闭设备测试closed facility test以及实地测试on-road test，不再能满足测试需求。</p>
<p>面对开发过程中的新挑战，虚拟仿真测试技术对自动车辆测试和验证起到了至关重要的作用。目前，虚拟测试已经成为了普遍接受的测试方法, 能够比传统测试在多方面测试有着更加优越的性能优势。作为虚拟测试的重要数据来源，自然驾驶数据上的大量收集使得公共交通数据集快速扩展，然而大量数据的组织形式缺乏可用性。为了实现虚拟测试在庞大交通数据集上进行大规模可拓展的测试，基于场景的测试方法被认为是可行的方法，它提供了一种基于交通场景理解的有效数据组织方式。场景测试具有自定义交通场景，真实交通场景的全面覆盖以及可在不同区域之间传输测试结果的优势。</p>
<p>尽管已经取得了很大的进步，但自动驾驶基于场景测试的过程中许多挑战仍然存在，例如测试场景数量众多，巨大的参数空间，这使得即便是基于场景的虚拟测试也将需要消耗大量的时间成本。为了提高测试的效率，研究人员对于测试中关键性（Critical）场景产生了极大的兴趣。</p>
<p>在场景测试中解决关键性场景挖掘的相关研究进展中，一种直接提高虚拟测试效率的方法是从大型的交通数据数据库中提取和分类相关场景，识别提取出关键场景生成相应的测试场景数据库；另一类方法是基于已有的数据集通过观察因果关系(based on observed cause-effect relations)来生成关键测试用例进行测试。本文主要想通过第一种方法，来探究了对于无人驾驶视觉感知任务中最差感知场景的搜索和提取。</p>
<p>我们主要关注于无人驾驶的视觉感知的测试问题，不同于其他动态驾驶任务（DDT）测试的困难，当前影响感知算法性能的主要原因在于训练数据和真实数据的分布不一致性，这主要反应为算法的泛化间隔（gap）。从机器学习的角度来看，对于任意算法，都存在一类或者多类最差实例，算法不能有效解决这类实例，如Bolte发现神经网络在训练集和最差样例数据集之间有着巨大的泛化隔阂。<strong>于是这些同类的实例在我们的工作中被定义为最差场景。更具体地来说, 最差感知场景可以被定义共享相同的场景参数的一个最差感知场景样本集。</strong>在我们的工作中最差感知场景（WPS）便是我们最为关注的关键性场景。</p>
<p>视觉场景测试的挑战也具有许多视觉方向的普遍挑战，例如困难关键数据的长尾效应问题，自然驾驶数据的组织形式缺乏可用性导致需求大量的领域专家人力标注成本等等问题。此外，具有极高维度的视觉场景特征和数量巨大且不连续的场景数据集都极大的增加了视觉感知场景测试的计算压力。在众多的实践中，我们发现通用的无人驾驶测试方法往往不能有效地解决这些视觉问题，我们需要采用视觉的相关解决方法来有效的处理这些问题和挑战。</p>
<p>于是针对自动驾驶系统的视觉感知任务，我们想要找到一个通用的最差感知场景的搜索框架。该框架能够结合不同的视觉动态驾驶任务（DDT），对不同的测试数据集设计相应的操作设计域（ODD），以完成最差感知场景的搜索。值得注意的是，关键性（最差）的定义是任务驱动的，我们在研究该问题的过程中主要以车辆检测算法为待测算法模型，以用测试结果的F1-score作为性能衡量标准。</p>
<h2 id="3-无人驾驶场景测试的背景与相关研究"><a href="#3-无人驾驶场景测试的背景与相关研究" class="headerlink" title="3. 无人驾驶场景测试的背景与相关研究"></a>3. 无人驾驶场景测试的背景与相关研究</h2><p>基于我们主要想要研究的问题我做了大量的调研和相关文章的总结</p>
<h3 id="3-1-无人驾驶场景测试的发展和分析"><a href="#3-1-无人驾驶场景测试的发展和分析" class="headerlink" title="3.1 无人驾驶场景测试的发展和分析"></a>3.1 无人驾驶场景测试的发展和分析</h3><p>随着自动驾驶智能等级的不断提高，任务需求的不断提高，许多因素，例如多变的天气、复杂交通环境、多样的驾驶条件等等都给自动驾驶测试带了更多的挑战，这使得传统测试方法不再满足无人驾驶测试的需求。在自动驾驶虚拟测试技术迅速发展的驱动下，基于场景的测试方法已被认为是可行的方法，由于基于场景的虚拟测试兼具可适应强、高效、可重复、低成本等优点，逐渐成为了无人驾驶测试和评估非常热点的研究方向。对于自动驾驶场景测试的研究主要在于探究如何评估不同场景下的自动驾驶性能表现,相关工作主要分为三个方面，分别是自动驾驶测试场景、自动驾驶测试平台以及自动驾驶加速测试方法。</p>
<p>然而，即使基于场景的虚拟测试中，研究者依然会面临测试场景数量众多和参数空间巨大所带来的较低的测试效率、大量消耗的计算资源的挑战。目前为止，研究者们来解决这个问题的方法主要有两种：</p>
<ul>
<li>一种直接方法是通过从大型交通数据库中提取和分类相关关键场景来提高测试效率。Galen E. Mullins 等人 使用自适应采样来搜索参数空间，然后通过无监督学习来确定性能模式和构成性能边界的情况。在2017年，Osama A. Osman等人使用vehicle kinematics数据集训练并比较分析了多种机器学习算法，包括K最近邻，随机森林，支持向量机，决策树，高斯朴素贝叶斯（Gaussian NB）和自适应提升（AdaBoost），对于车辆的预碰撞预测（near-crash prediction）的作用。在最近的工作中，Feng S等人提出了测试方案库生成问题的通用框架，为了搜索关键场景，设计了辅助目标函数，并应用了多步优化方法和种子填充。  </li>
<li>提高测试效率的另一类方法是基于现有数据观察到的因果关系来生成关键测试用例。在生成测试样例的早期研究中，Vânia de Oliveira Neves等人提出了支持自动驾驶汽车结构测试的环境，早期存储的测试场景增强通过组合和变异策略，生成新的测试用例。随后一些研究工作开始结合机器学习相关的自动生成方法，例如2018年Ian Rhys Jenkins等人提出了在基于循环神经网络来生成事故场景。此外其他的一些研究中，Christian Wolschke等人讨论了如何识别因果关系以及如何将这些关系组装成测试用例，用相应的分析结果来自动生成必要的测试用例，该工作同时也启发了Moritz Klischat等人的进一步研究，此工作中使用了进化算法，提出了基于待测车辆的解决方案空间最小化的关键场景自动生成方案。 </li>
</ul>
<p>我们想要展开的研究工作属于第一种方法，核心关注于最差感知场景的搜索和提取的研究。</p>
<h3 id="3-2-交通感知场景的研究发展"><a href="#3-2-交通感知场景的研究发展" class="headerlink" title="3.2 交通感知场景的研究发展"></a>3.2 交通感知场景的研究发展</h3><p>作为场景测试的核心和基础，自动驾驶场景的定义，收集和分析是评估测试的关键。自从Schieben等在无人驾驶测试领域应用了场景的概念，许多研究这都纷纷提出各自对于无人驾驶的理解。Jan-Aike Bolte等人提出了一个适用于自动驾驶的边缘样例（corner case）的定义；随后Jiajie Wang等人也提出了一种基于量化路段场景复杂度的交通感知数据分类范例。这些对于自动驾驶场景定义和分析的研究工作对后续研究工作的交通场景理解产生了积极的影响。</p>
<p>至今为止，在无人驾驶感知领域“场景”依然没有明确的统一的定义。在无人驾驶感知任务中，研究者往往基于交通场景的理解来定义视觉场景，以实现无人驾驶汽车能够对于不断变换的交通场景有所认识，包括天气，道路条件以及其他交通参与者交互。实现无人驾驶系统对于交通场景的理解的主要数据源头有两个：碰撞数据库和自然驾驶数据。</p>
<p>在自动驾驶视觉感知技术高速发展的驱动下，相关的自然驾驶视觉数据集也得到了不断地补充和扩展，许多关于道路场景复杂度的研究，也纷纷开始展开。最早的工作可以追溯到2013年的the Pascal VOC challenge， 这是在复杂交通场景下的图像识别挑战赛。随后，许多研究工作例如KITTI, RobotCar, Cityscapes，ApolloScape，BDD100K，等一系列工作提供了大量的真实交通场景的典型视觉样本。这些工作提供的大量人工标注包含了丰富的场景语义信息，为后来的视觉交通场景测试研究提供了坚实的数据基础。这些视觉交通场景数据的工作不仅极大地促进了无人驾驶视觉感知技术的快速发展，例如车辆检测、车道线识别、行人轨迹预测等等研究，同样也为测试评估工作提供了重要的场景来源。这些工作也为我们对于视觉交通最差感知场景的深入理解和认知提供了巨大的帮助。</p>
<h3 id="3-3-面向离散表征场景的搜索算法研究"><a href="#3-3-面向离散表征场景的搜索算法研究" class="headerlink" title="3.3 面向离散表征场景的搜索算法研究"></a>3.3 面向离散表征场景的搜索算法研究</h3><p>考虑到交通感知场景的参数空间巨大且离散的特点，我们希望把场景搜索的问题形式化为离散场景参数的搜索问题，这属于机器学习领域的经典问题之一。在近年来，许多离散参数搜索算法都被广泛的研究和使用，尤其是在神经架构搜索的研究中。贝叶斯优化作为最为流行的超参数搜索方法之一，在2013年Bergstra et al.[42]使用贝叶斯优化搜索得到了最先进的视觉架构（state-of-the-art vision architectures），但是由于经典的BO工具箱基于高斯过程和专注于低维的连续优化问题，所以并不适用于我们的工作。此外，进化算法也被用于网络架构参数搜索的优化，该工作中Real等人对强化学习、进化算法以及随机搜索进行了实验对比，得出强化学习和进化算法通常在最终表现会更好。通过对比强化学习和进化算法的形式化，我们发现近年来的对于神经网络架构搜索（NAS）问题的强化学习搜索策略结构更加适用于我们巨大搜索空间下的离散搜索问题。</p>
<p>值得注意的是，2017年Zoph and Le 通过基于强化学习结合循环神经网络（RNN）的搜索策略获得CIFAR-10和PennTreeBank基准测试的竞争性成绩后，把神经网络架构搜索问题（NAS）变成了机器学习社区的主流研究。基于强化学习的搜索策略后续得到了不断地改进和发展，Zoph等人在2018年进行了进一步地探究设计了一个具有可迁移能力地搜索空间。此外，该方法不仅在神经网络架构搜索（NAS）问题上取得了重大的成功，这个通用的搜索策略对其他搜索问题也能提供了极大的帮助。2017年，I. Bello等人使用该策略进行了网络优化器的搜索研究。随后google的研究团队也基于该策略提出了AutoAugment的研究工作。</p>
<p>受到这些研究工作启发，我们巧妙的把最差感知场景搜索问题通过交通场景离散化表征形式化为离散参数空间的搜索问题，找到了能有效的利用基于强化学习搜索策略的范式，并充分发挥该方法搜索高效、性能较好的优势。</p>
<h2 id="4-视觉感知场景的表征"><a href="#4-视觉感知场景的表征" class="headerlink" title="4. 视觉感知场景的表征"></a>4. 视觉感知场景的表征</h2><p>在第三部分中，我们分析了需要通过离散化表征场景才能有效地利用号强化学习结合循环神经网络的搜索范式，因此这一部分我着重讨论关于场景表征方式对于我们工作的影响和相关深入探讨。</p>
<h3 id="4-1-无人驾驶测试角度看待场景表征"><a href="#4-1-无人驾驶测试角度看待场景表征" class="headerlink" title="4.1 无人驾驶测试角度看待场景表征"></a>4.1 无人驾驶测试角度看待场景表征</h3><p>我们希望通过有效的视觉场景表征构建出感知场景空间，以完成场景标识和识别。在自动驾驶系统（ADS）的测试框架中，视觉场景表征实质上可以用来定义自动驾驶系统（ADS）视觉感知功能的部分操作设计域（ODD）。因此，自动驾驶系统（ADS）测试中操作设计域（ODD）的定义方法可以为视觉场景表征提供许多帮助和启发。</p>
<p>鉴于自动驾驶系统（ADS）技术的新兴性和高度竞争性，很难固有地获得有关自动驾驶系统（ADS）功能明确而完整的预期操作设计域（ODD）信息。在缺乏有关操作设计域（ODD）信息的情况下，有时会使用工程判断来定义操作设计域（ODD）分类法，并为概念自动驾驶系统功能识别操作设计域。 根据美国国家安全研究提出对于ODD识别和分类定义特别有用的信息来源主要来自5个方面，包括 产品描述书（Descriptions in the product literature），感知系统（Perception systems），视频图片信息（Videos and Image），事故报告（Testimonials） 以及源于其他领域知识的操作设计域（ODDs from other domains）。通过从其中3个不同比较有意义的角度分析视觉感知测试需求，我们可以得到实现有效视觉场景表征的启发。</p>
<h3 id="4-2-基于感知系统角度（Perception-systems）"><a href="#4-2-基于感知系统角度（Perception-systems）" class="headerlink" title="4.2 基于感知系统角度（Perception systems）"></a>4.2 基于感知系统角度（Perception systems）</h3><p>在视觉感知测试中，摄像头（camera）是主要的传感器套件，这确定了视觉感知任务中的一些关键自动驾驶系统ADS特征，例如光照强弱，天气条件，遮挡和截断等等。结合视觉领域知识我们知道，这些视觉相关的ADS特征其实表现为视觉场景的语义特征，通常这些特征都蕴含在利用专家知识完成的数据集标注中。从现在已有的一些自动驾驶场景数据集中(EVB, KITTI,BDD100K), 我们可以获得丰富的具有场景语义特征信息的标注。基于测试目标感兴趣的视觉感知任务，我们可以选择与道路交通场景动态驾驶任务（DDT）相关的场景元素作为视觉场景表征。 </p>
<p>在我们初步的工作中（IV2020），这些相关的场景特征分为两部分：</p>
<ul>
<li><p>场景的高层次理解特征</p>
<ul>
<li>天气条件</li>
<li>道路类型 </li>
<li>时间</li>
<li>交通参与者的数量</li>
<li>等..</li>
</ul>
</li>
<li><p>感兴趣目标的低层次属性 </p>
<ul>
<li>目标的遮挡程度</li>
<li>目标的截断程度</li>
<li>等..</li>
</ul>
</li>
</ul>
<p>我们利用这种场景表征方式对特征进行了离散化，构建了详细的场景参数空间，用以保证了场景在离散空间中的识别和分类以及后续我们搜索任务的顺利完成。</p>
<p>显然地，这种场景表征方式存在两个致命的<strong>缺点</strong>：</p>
<ol>
<li>一方面是该表征方法极其依赖数据集本身的标注。<ul>
<li>大部分交通数据集的标注不能满足任务需求甚至没有标注信息，而富有丰富场景语义特征的标注需要消耗大量的人力成本。</li>
</ul>
</li>
<li>另一方面，这些现有场景表征主要依靠领域专家根据实际工程需求来定义场景标注的标准，从而导致标准不一致，因此不同的数据集会包含不同类型的场景表征参数，甚至同一类型的场景参数的量化标准也存在不一致，例如一些交通数据集中遮挡级别只有2个级别，而另一些数据集中则更加细粒度的划分为详细的4个级别，这导致了数据集之间的场景表征不具有泛用性和一致性。</li>
</ol>
<h3 id="4-4-基于视频图片信息（Videos-and-Image）"><a href="#4-4-基于视频图片信息（Videos-and-Image）" class="headerlink" title="4.4 基于视频图片信息（Videos and Image）"></a>4.4 基于视频图片信息（Videos and Image）</h3><p>视频和图像为无人驾驶特定领域的测量提供了可视化文档（visual documentation），通常这些文档中包含着这些自动驾驶系统（ADS）功能中潜在的操作设计域（ODD）基础。在无人驾驶视觉感知的测试中，这些可视化文档就是测试评估的<strong>核心数据源</strong>，因此我们可以利用视觉数据本身的潜在特征来进行有效的场景表征。深度学习的快速发展为如何获取视视觉特征提供了许多高效的方法，因此利用神经网络自动提取的深度视觉特征来构建出新的场景空间是高效可行的方法。</p>
<p>针对场景表征的目标，我们使用在Place365数据集上完成了预训练的VGG模型对交通场景数据进行深度视觉特征的自动提取。通过最后一层的卷积池化层得到的深度特征，使用聚合得到了512维的特征表征。在自动提取深度视觉特征的过程中，我们摆脱了对于测试数据大量标注信息的依赖，极大的降低了人力标注的成本，而且即使对于不同的交通数据集，深度视觉特征的场景表征都具有通用性，使得场景表征具有较高的一致性。</p>
<p>虽然基于视觉特征的场景表征方式解决了基于语义特征的场景表征方式的致命缺点，但同时引入了深度学习视觉领域的一些特有的困难和难以避免的缺陷：深度学习的可解释性一直都是机器学习领域种亟待解决的研究难点。具体在该方法中，深度视觉特征导致了场景表征的可解释极差，人类无法理解场景表征的实际意义。此外我们可以发现深度视觉特征具有更高的维度，会构建出更大的场景参数空间.。巨大的参数空间对后续进一步的工作，例如场景识别、匹配和搜索等算法的性能和效率都提出了极高的要求。</p>
<h3 id="4-5-基于其他领域知识（ODDs-from-other-domains）"><a href="#4-5-基于其他领域知识（ODDs-from-other-domains）" class="headerlink" title="4.5 基于其他领域知识（ODDs from other domains）"></a>4.5 基于其他领域知识（ODDs from other domains）</h3><p>其实在4.4的场景表征方法中，为了获得有效的视觉特征，我们就引入了深度学习中有监督方法自动获取深度视觉特征的领域知识。然而我们依然需要一个完备的预训练数据集，该数据集需要有着相应的场景标签来支持有监督学习的模型训练。因此，我们希望进一步通过引入无监督方法获取视觉特征的领域知识，来获得场景表征以彻底摆脱对于数据集标注的依赖，这样的场景表征方法应该具有更强的普适性。</p>
<p>无监督方法中, 图像自编码器技术被普遍使用于降维以及特征学习，我们通过图像编码特征来进行视觉场景的表征。值得注意的是，图像编码特征由于包含了视觉数据更多的特征信息，因此会导致特征维度过高带来的维度灾难问题。我们需要进行有效的降维学习来实现特征维度达到我们能够接受的范围，同时还要保证特性信息最大程度的保留。为了满足要求，我们可以通过无监督的PCA线性降维方法降低图像编码的特征维度，在保证信息最大化保留的同时维持特征拓扑空间在降维后得到一个对应的线性流形。</p>
<p>对比第二种方法，我们可以发现基于图像自编码特征的场景表征依然具有的可解释性差，对后续算法性能等缺陷，但是可以采用相同解决方案使得该场景表征方式能最大程度的适用于后续无人驾驶场景测试评估。</p>
<h2 id="5-最差感知场景搜索框架"><a href="#5-最差感知场景搜索框架" class="headerlink" title="5.最差感知场景搜索框架"></a>5.最差感知场景搜索框架</h2><p>我们受到NAS领域的方法和Google的AutoAugment搜索范式的启发提出了一个解决最差感知场景的搜索框架，这个框架最初的基线模型（baseline model）就已经取得了不错的实验效果，但是这个模型仅仅只能使用与部分理想的实验环境下。于是我们对于该搜索框架进行了更进一步的改进，提出了性能更加优异的循环迭代框架。我们为了统一对比测试效果，我们选择车辆检测算法作为我们的待测算法。</p>
<h3 id="5-1-基线模型（baseline-model）"><a href="#5-1-基线模型（baseline-model）" class="headerlink" title="5.1 基线模型（baseline model）"></a>5.1 基线模型（baseline model）</h3><p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_1_20210423.png" srcset="/img/loading.gif"></p>
<p>由神经结构搜索框架（NAS）的启发，我们提出基线模型，如上图所示。总的来说，首先是循环神经网络（RNN）控制器从场景表征空间采样得到一个场景表征 S ；然后，将数据集中最接近目标场景 S 的样本集N检索出来得到一个场景样本集 N 。随后，在样本集 N 上测试的待测感知算法的性能表现，并转换为奖励值R；最后将奖励值 R 发送回控制器以更新循环神经网络（RNN）的网络参数。为了更新控制器的权重，我们使用强化学习中的策略梯度方法以解决奖励值 R 不可导的无法进行训练的问题。</p>
<h4 id="5-1-1-基于强化学习的搜索算法"><a href="#5-1-1-基于强化学习的搜索算法" class="headerlink" title="5.1.1 基于强化学习的搜索算法"></a>5.1.1 基于强化学习的搜索算法</h4><p>在这框架中，可以将控制器采样得到场景列表的过程视为一个动作列表 <strong>a1:T</strong>。车辆检测测试将从测试样本集上获得性能参数F1-score，通过简单的转化公式我们就可以获得奖励值 <strong>R</strong> ， 使用该奖励信号就可以通过强化学习方法训练控制器。奖励值的转换公式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_2_20210423.png" srcset="/img/loading.gif"></p>
<p>为了找到最差感知场景，控制器的训练目标是最小化采样场景最待测的特定车辆感知算法的性能表现。也就是说我们需要最大化整个蒙特卡洛过程的最大奖励期望值，期望值的公式可以表示为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_3_20210423.png" srcset="/img/loading.gif"></p>
<p>其中 <strong>θc</strong> 是控制器的网络参数。我们使用vanilla策略梯度方法来完成控制器的训练收敛。通常求梯度的经验近似公式表现为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_4_20210423.png" srcset="/img/loading.gif"></p>
<p>其中 <strong>m</strong> 是一个训练batch中被采样场景的个数；<strong>T</strong> 是我们控制器需要采样的场景的表征参数个数；奖励值 <strong>Rk</strong> 是 对应的第 <strong>k</strong> 个采样场景的奖励值。通常策略梯度算法也会引入一个基线函数来降低期望的方差。在我们的实现中，基线函数 <strong>b</strong> 是一个基于之前采样奖励值的指数滑动平均。</p>
<p>最后，策略梯度算法通过策略性能的随机梯度提升来更新控制器的参数，更新迭代式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_5_20210424.png" srcset="/img/loading.gif"></p>
<h4 id="5-1-2-循环神经网络控制器的结构"><a href="#5-1-2-循环神经网络控制器的结构" class="headerlink" title="5.1.2 循环神经网络控制器的结构"></a>5.1.2 循环神经网络控制器的结构</h4><p>控制器循环神经网络（RNN）架构的设计是由LSTM作为基本单元组成的网络。考虑到搜索空间和任务计算量的差异，每层中的隐藏单元的数量设置为60，为来自搜索空间的每个场景序列（其中 <strong>B</strong> 对应于场景参数的数量）保留 <strong>B</strong> 个  softmax输出预测。此外，场景的交叉熵损失由全部 <strong>B</strong> 个softmaxes的联合概率计算得出。对该交叉熵损失加正则化损失函数求导计算出控制器RNN的训练梯度。梯度最终会基于奖励值 <strong>R</strong> 进行缩放以更新控制器RNN，使得控制器为更低的概率采样到简单场景，更高的概率采样到最差场景。</p>
<p>针对控制器的训练，超参数设置如下：</p>
<ul>
<li>RMSProp优化器<ul>
<li>学习率0.001</li>
<li>学习率的指数衰减系数0.95，衰减步长50</li>
</ul>
</li>
<li>基线函数<ul>
<li>指数滑动平均值的过去奖励值权重为0.8</li>
</ul>
</li>
<li>控制器初始化参数<ul>
<li>均匀地初始化为-0.1和0.1</li>
</ul>
</li>
<li>场景采样探索率<ul>
<li>权重为0.5</li>
</ul>
</li>
</ul>
<h4 id="5-1-3-场景匹配器的样本采样方法"><a href="#5-1-3-场景匹配器的样本采样方法" class="headerlink" title="5.1.3 场景匹配器的样本采样方法"></a>5.1.3 场景匹配器的样本采样方法</h4><p>为了获得在目标场景下检测器的测试性能，我们需要从数据集中找到最匹配采样得到的目标场景的样本集来完成进一步的预测。通过K-nearest neighbor算法的思想，我们找到 n 个在场景参数空间中距离目标场景最接近的样本作为匹配样本集。默认情况下，我们认为场景参数空间中的每个场景表征元素对距离具有相同的权重（每个场景描述元素对性能的影响被认为是等效的），因此我们需要对场景空间进行归一化处理。</p>
<p>因此样本和目标场景之间的距离 <strong>D</strong> 可以表示为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_6_20210424.png" srcset="/img/loading.gif"></p>
<h3 id="5-2-改进模型"><a href="#5-2-改进模型" class="headerlink" title="5.2 改进模型"></a>5.2 改进模型</h3><p>在基线模型中，我们基于来自数据集的场景语义标签特征进行场景表征，用于构建出全部测试数据所共享的离散场景搜索空间作为操作设计域（ODD）。特别地，利用强化学习结合LSTM实现高效的场景搜索。值得注意的是该方法需要满足两个理想假设：</p>
<ul>
<li>实验数据集带有较为准确且丰富的场景语义标签</li>
<li>场景空间中任意一个场景都能从数据集中匹配到近似的样例</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_7_20210424.png" srcset="/img/loading.gif"></p>
<p>为了缓解这两个假设对于此方法的泛用性和应用价值的限制，我们进一步的改进了整个框架，如上图所示.。具体的修改主要为两个部分：</p>
<ul>
<li>为了进一步提高匹配数据集的内部场景一致性，我们引入了重排序技术（reranking）用以改进匹配器的性能。</li>
<li>我们增加了判别器来鉴别匹配器得到的数据是否真的有效命中，避免场景空间数据稀疏导致的强制匹配出现。</li>
</ul>
<p>于是总体来看，我们提出的改进框架拥有三个核心部件：控制器、匹配器和判别器，而框架正常执行之前，需要完成基于场景表征的离散场景空间构建，以供控制器采样和匹配器检索。</p>
<h4 id="5-2-1-场景匹配器的重排序算法"><a href="#5-2-1-场景匹配器的重排序算法" class="headerlink" title="5.2.1 场景匹配器的重排序算法"></a>5.2.1 场景匹配器的重排序算法</h4><p>我们发现基于视觉特征的巨大场景空间导致测试数据集在部分场景空间分布非常的稀疏，如果我们依然使用之前基于KNN的匹配器进行强制数据匹配，会极大的降低了匹配数据集的匹配准确性和内部一致性。因此，我们想要引入图像检索领域的重排序技术，通过Re-ranking with K-reciprocal可以极大的改善我们遇到的困难。需要注意的是重排序对于内存和计算时间的消耗是比较高的，在迭代中动态地完成匹配的重排序会极大的降低我们搜索效率。</p>
<p>结合重排序的优缺点，我们提出了一个改进的新匹配器。首先我们需要对整个测试数据集进行静态的重排序作为前置工作，也就是测试集中全部样本的场景表征都需进行一次重排序计算，并把全部排序结果保存为排序表作为搜索框架的静态资源 , 如下图的算法流程<strong>Algorithm 1</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_8_20210424.png" srcset="/img/loading.gif"></p>
<p>虽然重排序依然所带来的巨大的计算消耗，但是整个排序过程从搜索的迭代中解耦了出来，框架在搜索过程中依然能保持着极高的迭代效率。随后我们需要通过基于KNN的代理（proxy）去查询排序表中最接近目标场景的样本场景表征对应排序列表（list）就可以得到相应的重排序结果, 如下图的算法流程<strong>Algorithm 2</strong>。代理的实现原理和基于KNN的匹配器基本相同，因此在搜索循环中匹配计算量和运行效率也基本保持一致。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_9_20210424.png" srcset="/img/loading.gif"></p>
<h4 id="5-2-2-新引入的判别器"><a href="#5-2-2-新引入的判别器" class="headerlink" title="5.2.2 新引入的判别器"></a>5.2.2 新引入的判别器</h4><p>引入重排序很大程度的提高了匹配数据的内部一致性和匹配准确性，但是这个改善的能力是有一定限度的。如果测试数据集在目标场景附近分布过于稀疏甚至完全缺失，那么即使改进后的匹配器依然无法保证匹配准确性。在本工作中，我们提出了一个匹配样本集的判别器，用来判别匹配器获得的样本集是否有效。如果样本集的类中心和目标场景在场景空间的距离超过了设定阈值，就跳过控制器的训练，让控制器参数和之前保持不变进行重采样，获得新的目标场景，进入下一轮迭代。这样可以有效地避免无效目标场景对控制器训练的干扰。下面的<strong>Algorithm 3</strong>描述了判别器的工作流程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_10_20210424.png" srcset="/img/loading.gif"></p>
<h2 id="6-最差感知场景搜索框架的基本实验"><a href="#6-最差感知场景搜索框架的基本实验" class="headerlink" title="6. 最差感知场景搜索框架的基本实验"></a>6. 最差感知场景搜索框架的基本实验</h2><h3 id="6-1-场景搜索框架的验证实验"><a href="#6-1-场景搜索框架的验证实验" class="headerlink" title="6.1 场景搜索框架的验证实验"></a>6.1 场景搜索框架的验证实验</h3><p>为了充分和深刻地证明我们框架的有效性，我们在多个不同的数据集完成了搜索实验，包括EVB，KITTI，以及 BDD100K。此外，在每个数据集上，测试了不同的车辆检测网络，例如YOLOv3，Faster-RCNN 以及 RetinaNet （在COCO dataset上进行预训练）。</p>
<h4 id="6-1-1-基于有标注数据集的场景搜索实验"><a href="#6-1-1-基于有标注数据集的场景搜索实验" class="headerlink" title="6.1.1 基于有标注数据集的场景搜索实验"></a>6.1.1 基于有标注数据集的场景搜索实验</h4><p>结合数据集丰富的语义标注，我们可以依照4.2中的分析，基于视觉场景的语义特征完成测试数据集的场景表征并构建出离散场景空间。具体的场景空间如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_11_20210424.png" srcset="/img/loading.gif"></p>
<p>由于EVB数据集中只有1400个图像样本，因此在EVB实验中场景匹配器的样本集数量设置为5，以确保场景之间的样本相关性。在KITTI和BDD100K实验中，由于样本数量充足，场景匹配器的样本集数量设置为10个样本，用于构建采样样本集，以提高样本集的表征目标场景的稳定性。对于要进行测试的所有车辆检测器，OU阈值以及类概率的阈值被设定为0.5。</p>
<p>在我们的框架中，控制器可以在CPU上，短时间有效地完成训练收敛。在EVB和Kitti上，控制器训练得到一个搜索场景大约5到8分钟。在BDD100K上，由于场景匹配器所需的计算量增加，因此需要大约10到15分钟。所有实验均使用Tensorflow完成。为了消除特定模型泛化性能的不确定性，每个数据集的实验重复200次。</p>
<ul>
<li><p>在基线模型上我们的搜索结果（场景匹配器KNN）</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_12_20210424.png" srcset="/img/loading.gif"></p>
</li>
<li><p>在改进模型上我们的搜索结果（补充个GT来作为性能参照）（场景匹配器reranking）</p>
<ul>
<li>我们使用待测算法对测试数据集中全部样本的场景表征都进行了匹配和推理，以获得全部测试数据场景的平均性能和最差性能作为Ground truth。 </li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_13_20210424x.png" srcset="/img/loading.gif"></p>
</li>
</ul>
<p>值得注意的是，虽然我们证明了在理想条件下，本工作提出的基本搜索框架是高效可行的，但是在实际的感知测试过程中，往往很难保证数据集具有完备的标注，而且并且每次采样到的目标场景都一定在测试数据集中有匹配的样本数据。因此我们需要在没有理想假设的条件下完成进一步的实验探究。</p>
<h4 id="6-1-2-基于无标注数据集的的场景搜索"><a href="#6-1-2-基于无标注数据集的的场景搜索" class="headerlink" title="6.1.2 基于无标注数据集的的场景搜索"></a>6.1.2 基于无标注数据集的的场景搜索</h4><p>为了适应各种不同的测试评估条件摆脱理想假设，我们选取了不具备丰富场景语义标注的数据集Apolloscape作为测试数据集。值得注意的是，为了证明我们方法同样适用于人工生成的测试数据集，我们通过雾霾场景生成方法，使用Apolloscape作为原始数据集生成相应的雾霾数据，得到了由原始数据和生成数据共同组成的雾霾数据集。于是我们在原始数据集和雾霾数据集上同时进行了搜索实验。本实验中，我们依然选择了车辆检测算法YOLOv3作为待测模块，并在改进模型上完成整个搜索实验。</p>
<h5 id="场景空间构建"><a href="#场景空间构建" class="headerlink" title="场景空间构建"></a>场景空间构建</h5><p>由于Apolloscape中标注不具有丰富的场景语义特征，我们很难基于语义特征实现充分的场景表征。结合4.4和4.5的分析，我们分别引入了两种不同的深度视觉特征进行场景表征：</p>
<ul>
<li>第一种方法是我们使用在Place365[49]上完成了预训练的VGG[50]模型对测试数据集进行深度视觉特征的自动提取，使用gap聚合最后一层的卷积池化层的特征，得到512维的特征表征</li>
<li>另一种方法是我们使用图像自编码器，使用的内容编码器获得图像编码作为特征表征。</li>
</ul>
<p>为了避免深度视觉特征带来了高维度的巨大计算消耗，对两种特征表征我们在保留绝大部分特征信息的基础上，统一采用PCA线性降维到了25维，并离散化和归一化到-1到1的范围。最后，为两种不同场景表征拼接上相同的5个语义特征（apolloscape标注所能提供的有限的语义信息），包括车数vehicle，行人数person，非机动车数non-motor，车流group，是否是生成雾霾场景haze，我们可以构建出两个不同的离散场景空间，都具有30维的特征。</p>
<h5 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h5><p>考虑到表征维度的提高以及数据在部分场景空间的稀疏分布，我们使用reranking改进的匹配器完成样本匹配，并且使用了鉴别器。基于实验经验，场景匹配器的样本集数量设置为10个样本。 鉴别器的距离阈值6.5。 其他参数设置和6.1.2上一个实验保持相同。</p>
<h5 id="实验流程设置"><a href="#实验流程设置" class="headerlink" title="实验流程设置"></a>实验流程设置</h5><p>在我们的框架中，虽然新匹配器的重排序需要消耗大量的计算资源和时间，但实际上我们通过基于KNN的代理去匹配预排序的静态资源仍然保证了整个搜索过程高效运行。训练控制器在大约10到15分钟内能搜索出最差感知场景。 为了消除特定模型的泛化性能不确定性，每个数据集上的实验都重复了100次。</p>
<h5 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h5><p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_14_20210424.png" srcset="/img/loading.gif"></p>
<p>在这部分实验中，我们对Apolloscape的原始数据集和雾霾数据集分别在两种不同场景空间中进行最差场景搜索实验。为了证明改进框架能够能在无理想假设的条件下同时适用于大部分自然测试数据集和生成数据集，和之前的方法一样我们对两个不同数据集中全部测试样本的场景表征的平均性能和最差性能作为GT。每一个实验我们重复100次搜索，分别对搜索到的平均和最差性能用来进行对比。</p>
<p>总的来看，无论是在原始数据集还是雾霾数据集，最差感知场景的平均性能远低于测试数据集全部样本场景的平均性能，而且搜索到的最差性能也非常接近全局最优解。这证明了我们的搜索框架对于两种不同视觉特征的引入都能有效解决数据集标注信息不充分或者缺失情况下场景表征的问题，进而顺利高效地完成搜索目标。并且不仅适用于自然场景数据集，同样也适用于生成场景数据集，具有很高的泛用性。</p>
<p>更具体地，我们对比两种不同场景空间下的结果可以发现，基于图像编码特征的结果整体平均性能相较于VGG自动提取特征的结果更低，甚至能稳定找到最差场景的真值。那么可以认为基于无监督的图像编码特征实现场景表征可以实现更好的搜索性能。</p>
<h4 id="6-1-3-对比两个模型的消融实验"><a href="#6-1-3-对比两个模型的消融实验" class="headerlink" title="6.1.3 对比两个模型的消融实验"></a>6.1.3 对比两个模型的消融实验</h4><ul>
<li><p>实验一：匹配器的改进对比</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_15_20210425.png" srcset="/img/loading.gif"></p>
<p>为了证明匹配器在引入重排序技术后匹配样本和目标场景的匹配精确度更高、内部一致性更强，在Apolloscape上我们分别使用基于KNN的匹配器和基于代理完成重排序的匹配器进行相同目标场景的匹配结果对比，具体示例上图。</p>
<p>显然地，我们可以看出基于重排序的匹配器的匹配精确度远高于基于KNN的匹配器。并且我们使用全部匹配样本到它们类中心点的平均距离来衡量他们的内部一致性，同样也是基于reranking的匹配器具有显著的优势</p>
</li>
</ul>
<ul>
<li><p>实验二：模型搜索性能对比</p>
<ul>
<li> 基线模型实验结果</li>
</ul>
<p>  <img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_17_20210425.png" srcset="/img/loading.gif"></p>
<ul>
<li> 改进模型实验结果</li>
</ul>
<p>  <img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_16_20210425.png" srcset="/img/loading.gif"></p>
<p>为了证明基于重排序的匹配器结合鉴别器有效地避免了无有效匹配数据的目标场景对于控制器训练的消极影响。我们补充了基于KNN匹配器的下默认全部命中的最差场景搜索实验。特别需要注意的是，该实验的全部设置都需要和6.1.2中的设置完全一致，以实现有效的对照。具体实验结果上图对比。</p>
<p>相比之前的结果，我们可以发现该实验中最差感知场景的平均性能相比较于Fig的结果有了明显的提高，而且在基于预训练vgg的深度特征的场景空间中搜索到的最差性能也有所提高。因此如果我们不使用改进后的匹配器结合鉴别器，无有效数据命中的目标场景对于搜索实验的干扰是很严重的，会明显降低我们搜索的结果性能。</p>
</li>
</ul>
<h2 id="7-最差感知场景搜索实验的拓展和发现"><a href="#7-最差感知场景搜索实验的拓展和发现" class="headerlink" title="7. 最差感知场景搜索实验的拓展和发现"></a>7. 最差感知场景搜索实验的拓展和发现</h2><h3 id="7-1-最差感知场景的困难性和迁移性"><a href="#7-1-最差感知场景的困难性和迁移性" class="headerlink" title="7.1 最差感知场景的困难性和迁移性"></a>7.1 最差感知场景的困难性和迁移性</h3><ul>
<li><p>困难性</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_18_20210425.png" srcset="/img/loading.gif"></p>
<p>基于第6部分的场景搜索基本实验，我们找到了匹配最差感知场景的样本集进行性能测试分布分析，来观察匹配样本是否都能满足是困难样本，具体分布如上图。场景scenario1是针对YOLOv3检测器，在EVB数据集上随机选择的一个搜索结果。为了比较，我们也绘制了最差性能样本对应的场景的性能分布（非最差感知场景）。</p>
<p>我们可以发现，最差感知场景可以基于场景有效地从数据集中匹配hard样本，而最差性能样本对应的场景却没有这样的特性。这与我们最差感知场景的定义特征一致。</p>
</li>
</ul>
<ul>
<li><p>迁移性</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_19_20210425.png" srcset="/img/loading.gif"></p>
<p>为了探讨在具有相同分布特性的数据集上最差感知场景（WPS）的泛化能力，我们试着对比了最差感知场景在训练集和验证集之间的迁移表现。我们选择了具有数据量较大的BDD100K，将其分为两个独立且同分布的训练集和验证集。通过对比上表的结果，我们可以发现框架在BDD100K训练集上搜索出的最差感知场景（WPS）也适用于BDD100K的验证集，反之亦然。</p>
<p>这个结果表面最差感知场景在同分布的数据集之间具有良好的可迁移特性，突出显示我们的搜索框架在大型数据集中的更高的工作价值，即可以选择近似分布中的一小部分数据来代替完整数据集，进行最差感知场景搜索，这可以极大降低计算成本。</p>
</li>
</ul>
<h3 id="7-2-不同场景表征性能差距的原因分析"><a href="#7-2-不同场景表征性能差距的原因分析" class="headerlink" title="7.2 不同场景表征性能差距的原因分析"></a>7.2 不同场景表征性能差距的原因分析</h3><p>我们在6.1.2的实验中可以发现，基于内容编码特征完成场景表征的搜索效果显著的优于基于VGG预训练提取特征实现场景表征的效果。为了深入分析两种基于视觉特征的场景表征性能存在差异的原因，我们使用t-sne可视化了两个不同场景空间的部分数据分布情况。从下图中我们可以发现，基于图像编码特征的连续场景数据，其分布也表现出连续性，这也使得困难场景更加有规律的聚集在一起，有利于搜索算法的更加稳定地收敛，因此可以获得更好的搜素性能。至于实验结果背后的本质视觉问题，我们将在未来的工作中进一步探究。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_20_20210425.png" srcset="/img/loading.gif"></p>
<h3 id="7-3-最差感知场景搜索结果的规律"><a href="#7-3-最差感知场景搜索结果的规律" class="headerlink" title="7.3 最差感知场景搜索结果的规律"></a>7.3 最差感知场景搜索结果的规律</h3><p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_21_20210425.png" srcset="/img/loading.gif"></p>
<p>为了凸显实验中最差感知场景搜索结果的规律，我们把基于有标注数据集的场景搜索结果总结在了上图。</p>
<ol>
<li>从结果中，我们发现车辆探测器的遮挡和截断参数对所有数据集上进行搜索实验都产生了显著的性能影响。</li>
<li>此外，通过观察场景参数中的数量参数，例如，汽车数量，行人数量等，一般表现为数量较大的趋势，这表明具有更多参与者的拥塞场景通常对于车辆探测器都是最差感知场景。</li>
<li>特别地，我们发现EVB数据集上的实验中，相比多种不同配置的基于 Faster-RCNN的车辆检测器，三个基于RetinaNet网络结构的检测器的搜索结果之间相似度较高。此外，也可以在KITTI的实验结果统计中清楚地发现这种特性，每个基于RetinaNet的车辆检测器的搜索结果都基本相同，每个基于Faster-RCNN的车辆检测器的搜索结果都基本相同。基于这一发现，可以得出初步结论：相似的网络模型的最差感知场景（WPS）也更为相似，这表明相似网络模型对车辆检测任务的场景感知具有一定的一致性。</li>
</ol>
<h2 id="8-最差感知场景搜索框架的应用实验（困难场景库生成）"><a href="#8-最差感知场景搜索框架的应用实验（困难场景库生成）" class="headerlink" title="8. 最差感知场景搜索框架的应用实验（困难场景库生成）"></a>8. 最差感知场景搜索框架的应用实验（困难场景库生成）</h2><p>林子航师弟接手了一个基于表征解离的雾霾图像生成方法的工作，为了充分、有力地证明生成雾霾图像通过对雾霾场景长尾数据的生成为无人驾驶感知模块的场景测试提供了有效的困难场景数据支撑，我们设置了对于生成雾霾图像加入前后的数据集中最差场景搜索的对比实验，该实验用于论证我们最差感知搜索框架可以应用于困难场景集压力分析和困难场景库生成工作。</p>
<h3 id="8-1-雾霾图像对于场景测试困难程度影响的规律性探究"><a href="#8-1-雾霾图像对于场景测试困难程度影响的规律性探究" class="headerlink" title="8.1 雾霾图像对于场景测试困难程度影响的规律性探究"></a>8.1 雾霾图像对于场景测试困难程度影响的规律性探究</h3><p>为了说明雾霾图像生成对于场景测试难度影响的内在规律性，我们选择了5773个Apolloscape上的连续道路场景数据作为了原始数据集，其中全部都是含有车辆的，然后我们生成对应的5773张最强浓度的雾霾场景图像加上原始数据集数据构成了一个11546个数据构成的完整雾霾数据集。特别地，为了保证生成数据和原始数据保持一致性，我们把原始数据地分辨率下降到和生成图片一致来进行实验。考虑到分辨率大幅度下降地原因，在原始数据转换后我们把目标bbox长宽都小于5个像素的车辆真值进行了剔除。</p>
<p>对于最差感知场景搜索空间的设置，我们选择由5个场景级的场景特征参数（机动车数量、行人数量、非机动车数量、人群数量以及雾霾浓度风格编码）和25个内容编码器获得的内容编码（对内容编码进行PCA降维到25维）作为搜索空间，注意雾霾参数只有0和1分别表示原始数据和生成雾霾数据。对于最差感知场景搜索框架，我们设置每次搜索的迭代次数为300次，并在原始数据集和雾霾数据集上分别进行了100次最差感知场景搜索实验。</p>
<p>对两个数据集的整体性能表现和最差感知场景的搜索结果进行了对比，结果如下表。雾霾数据集的整体性能相比于原始数据集的整体表现有大幅度的下降，而且两个数据集的最差感知场景整体性能也体现出了相同的趋势。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_22_20210425.png" srcset="/img/loading.gif"></p>
<p>再结合最差感知场景的性能分布图，如下图，我们可以清晰的观察到最差感知场景的性能变化的分布规律，最差感知场景的困难性得到了显著的提升。以上实验结果充分地表明生成雾霾图像对于数据集的整体困难度是有着明显提升的，而且甚至本身是困难场景的数据也具有相同的趋势，说明了生成雾霾的方法对于数据困难度提升具有一致性，对于数据本身的困难程度上界也有明显的提升。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_23_20210425.png" srcset="/img/loading.gif"></p>
<h3 id="8-2-雾霾图像对于场景测试困难程度影响的关联性探究"><a href="#8-2-雾霾图像对于场景测试困难程度影响的关联性探究" class="headerlink" title="8.2 雾霾图像对于场景测试困难程度影响的关联性探究"></a>8.2 雾霾图像对于场景测试困难程度影响的关联性探究</h3><p>为了进一步探究两个数据集中最差感知场景的关联性，我们需要对比在两个数据集上最差感知场景相互迁移性能的实验结果。在原始数据全是无雾霾图像，搜索空间中的雾霾浓度只有一个参数，所以雾霾参数的影响就消失了，那么我们只要保持其余场景参数不变，把雾霾数据集的最差场景直接迁移到原始数据集上匹配数据并进行性能测试以实现场景迁徙的目的；反过来，原始数据集的最差感知场景也直接迁移到雾霾数据集进行数据匹配并通过测试获得性能表现，此外我们又增加了其他场景参数保持不变，雾霾浓度参数修改为有雾霾1的实验，进行对比。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_24_20210425.png" srcset="/img/loading.gif"></p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_25_20210425.png" srcset="/img/loading.gif"></p>
<p>根据上面，迁徙前后的分布变化图，我们可以发现：</p>
<ul>
<li>原始数据集的最差感知场景在雾霾数据集下依然表现出很高甚至更高的困难性，而且其性能分布也是依然保持一致困难性的，这很清晰的表明原始数据集中的最差感知场景在雾霾数据集中依然是最差感知场景；</li>
<li>而雾霾数据集上的最差感知场景迁移到原始数据集上的性能相比较于原始数据集本身的最差场景性能反而有着一定的提升，而整体性能分布图反而出现了并不是最差感知场景的数据分布，这一定程度上说明了雾霾数据的最差感知场景并不一定在原始数据集中就一定是最差感知场景。</li>
</ul>
<p>我们可以得到一个统一结论：雾霾数据集的最差感知场景是包含原始数据集场景的，而且通过生成雾霾我们获得了更多额外的最差感知场景，说明了生成雾霾图像对于提升数据集的困难场景丰富性也有着重要的作用。</p>
<h3 id="8-3-生成雾霾浓度可控变化在场景的自适应测试应用中的合理性探究"><a href="#8-3-生成雾霾浓度可控变化在场景的自适应测试应用中的合理性探究" class="headerlink" title="8.3 生成雾霾浓度可控变化在场景的自适应测试应用中的合理性探究"></a>8.3 生成雾霾浓度可控变化在场景的自适应测试应用中的合理性探究</h3><p>为了探究雾霾浓度可控变化对于数据困难程度的可控性和合理性，我们在原始数据集的基础上，除了增加最高浓度的对应生成雾霾图像以外，我们还增加了各种不同雾霾浓度的生成图像，包括0.25，0.5，0.625，0.75，0.875。其余的数据处理与之前基本相同，那么我们可以得到由40411张图像所构成的不同雾霾浓度的新搜索数据集，这里称为多浓度雾霾数据集。</p>
<p>最差感知场景搜索空间依然与之前的设置基本保持不变，但是场景级特征参数之一的雾霾参数要从（0，1）转换为（0，0.25，0.5，0.625，0.75，0.875，1）。最差感知场景的搜索迭代次数依然设置为300，我们对该数据集进行125次最差感知场景搜索实验。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_26_20210425.png" srcset="/img/loading.gif"></p>
<p>从上图我们可以发现在多浓度雾霾数据集中最差感知场景的雾霾浓度参数大部分都分布在高浓度的雾霾图像当中，因此我们可以认为雾霾浓度的变化对于数据困难度的影响正相关的。</p>
<p>为了进一步论证场景数据中最差感知场景与雾霾浓度的相关性，我们通过人类可理解的5个场景级参数与相对应的最差感知场景出现次数的相关系数来衡量这些场景参数与最差感知场景的相关性，具体如下表。我们可以发现雾霾浓度参数与最差感知场景出现次数的皮尔逊系数是大于0.8，说明是具有强线性相关的。而与我们以往工作中发现车辆数量和行人数量等交通参与者应该也是和最差感知强相关结论不同，这里的相关性就相对很弱了，甚至是负相关，我认为应该是数据集分辨率下降后我们对小目标真值进行剔除所导致的去相关。依据雾霾浓度与最差感知场景的强相关性，我们可以认为通过改变雾霾浓度来实现场景测试的自适应难度变化是合理性和可靠性的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/LihengXu/mycdn/img/%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93_27_20210425.png" srcset="/img/loading.gif"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Beglerovic H, Stolz M, Horn M. Testing of autonomous vehicles using surrogate models and stochastic optimization[C]//2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2017: 1-6
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>S. O.-R. A. V. S. Committee et al. Taxonomy and definitions for terms related to driving automation systems for on-road motor vehicles. SAE International: Warrendale, PA, USA, 2018.
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Thorn E, Kimmel S C, Chaka M, et al. A framework for automated driving system testable cases and scenarios[R]. United States. Department of Transportation. National Highway Traffic Safety Administration, 2018.
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>J. M. Anderson, K. Nidhi, K. D. Stanley, P. Sorensen, C. Samaras, and O. A. Oluwatola. Autonomous vehicle technology: A guide for policymakers. Rand Corporation, 2014.
<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>M. Ardelt, C. Coester, and N. Kaempchen. Highly automated driving on freeways in real traffic using a probabilistic framework. IEEE Transactions on Intelligent Transportation Systems, 13(4):1576–1585, 2012.
<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>F. Saust, J. M. Wille, B. Lichte, and M. Maurer. Autonomous vehicle guidance on braunschweig’s inner ring road within the stadtpilot project. In 2011 IEEE Intelligent Vehicles Symposium (IV), pages 169–174. IEEE, 2011.
<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>C. Urmson, J. Anhalt, D. Bagnell, C. Baker, R. Bittner, M. Clark, J. Dolan, D. Duggins, T. Galatali, C. Geyer, et al. Autonomous driving in urban environments: Boss and the urban challenge. Journal of Field Robotics, 25(8):425–466, 2008.
<a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Klischat M, Althoff M. Generating critical test scenarios for automated vehicles with evolutionary algorithms[C]//2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019: 2352-2358.
<a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>B. Kim, Y . Kashiba, S. Dai, and S. Shiraishi, “Testing autonomous vehicle software in the virtual prototyping environment,” IEEE Embedded Systems Letters, vol. 9, no. 1, pp. 5–8, 2017.
<a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>M. R. Zofka, S. Klemm, F. Kuhnt, T. Schamm, and J. M. Zöllner, “Testing and validating high level components for automated driving: Simulation framework for traffic scenarios,” in Proc. of the IEEE Intelligent V ehicles Symposium, 2016, pp. 144–150.
<a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>R. Math, A. Mahr, M. M. Moniri, and C. Müller, “OpenDS: A new open-source driving simulator for research,” in Proc. of Automotive meets Electronics, 2013.
<a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>Zhao D, Guo Y, Jia Y J. Trafficnet: An open naturalistic driving scenario library[C]//2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2017: 1-8.
<a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>A. Pütz, A. Zlocki, J. Küfen, J. Bock, and L. Eckstein, “Database approach for the sign-off process of highly automated vehicles,” in 25th International Technical Conference on the Enhanced Safety of Vehicles (ESV) National Highway Traffic Safety Administration, 2017.
<a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:14" class="footnote-text"><span>J. V esterstrom and R. Thomsen, “A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems,” in Proc. of the Congress on Evolutionary Computation, vol. 2, 2004, pp. 1980–1987.
<a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:15" class="footnote-text"><span>G. E. Mullins, P . G. Stankiewicz, and S. K. Gupta, “Automated generation of diverse and challenging scenarios for test and evaluation of autonomous vehicles,” in Proc. of the IEEE International Conference on Robotics and Automation, 2017, pp. 1443–1450
<a href="#fnref:15" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:16" class="footnote-text"><span>C. Wolschke, D. Rombach, P . Liggesmeyer, and T. Kuhn, “Mining test inputs for autonomous vehicles,” in Proc. of Commercial Vehicle Technology, 2018, pp. 102–113.
<a href="#fnref:16" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:17" class="footnote-text"><span>V. De Oliveira Neves, M. E. Delamaro, and P . C. Masiero, “An environment to support structural testing of autonomous vehicles,” in European Signal Processing Conference, 2014, pp. 19–24.
<a href="#fnref:17" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:18" class="footnote-text"><span>I. R. Jenkins, L. O. Gee, A. Knauss, H. Yin, and J. Schroeder, “Accident scenario generation with recurrent neural networks,” in Proc. of IEEE Conf. on Intelligent Transportation Systems, 2018, pp.3340–3345
<a href="#fnref:18" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:19" class="footnote-text"><span>Feng S, Feng Y, Yu C, et al. Testing scenario library generation for connected and automated vehicles, part i: Methodology[J]. IEEE Transactions on Intelligent Transportation Systems, 2020.
<a href="#fnref:19" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:20" class="footnote-text"><span>Osman, O.A., Hajij, M., Bakhit, P.R., Ishak, S., 2019. Prediction of Near-Crashes from Observed Vehicle Kinematics using Machine Learning. Transportation Research Record 2673(12), 463-473.
<a href="#fnref:20" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:21" class="footnote-text"><span>Perez, M.A., Sudweeks, J.D., Sears, E., Antin, J., Lee, S., Hankey, J.M., Dingus, T.A., 2017. Performance of basic kinematic thresholds in the identification of crash and near-crash events within naturalistic driving data. Accident Analysis &amp; Prevention 103, 10-19.
<a href="#fnref:21" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:22" class="footnote-text"><span>Bolte, J., Bar, A., Lipinski, D., Fingscheidt, T., 2019. Towards Corner Case Detection for Autonomous Driving. arXiv: Computer Vision and Pattern Recognition.
<a href="#fnref:22" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:23" class="footnote-text"><span>S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.
<a href="#fnref:23" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:24" class="footnote-text"><span>M. Ardelt, C. Coester, and N. Kaempchen. Highly automated driving on freeways in real traffic using a probabilistic framework. IEEE Transactions on Intelligent Transportation Systems, 13(4):1576–1585, 2012.
<a href="#fnref:24" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:25" class="footnote-text"><span>C. Zhang, Y. Liu, D. Zhao, and Y. Su. Roadview: A traffic scene simulator for autonomous vehicle simulation testing. In 17th International IEEE Conference on Intelligent Transportation Systems (ITSC), pages 1160–1165. IEEE, 2014.
<a href="#fnref:25" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:26" class="footnote-text"><span>D. Zhao, Y. Liu, C. Zhang, and Y. Li. Autonomous driving simulation for unmanned vehicles. In 2015 IEEE Winter Conference on Applications of Computer Vision, pages 185–190. IEEE, 2015.
<a href="#fnref:26" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:27" class="footnote-text"><span>M. R. Zofka, S. Klemm, F. Kuhnt, T. Schamm, and J. M. Z¨ollner. Testing and validating high level components for automated driving: simulation framework for traffic scenarios. In 2016 IEEE Intelligent Vehicles Symposium (IV), pages 144–150. IEEE, 2016.
<a href="#fnref:27" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:28" class="footnote-text"><span>J. M. Anderson, K. Nidhi, K. D. Stanley, P. Sorensen, C. Samaras, and O. A. Oluwatola. Autonomous vehicle technology: A guide for policymakers. Rand Corporation, 2014.
<a href="#fnref:28" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:29" class="footnote-text"><span>F. Saust, J. M. Wille, B. Lichte, and M. Maurer. Autonomous vehicle guidance on braunschweig’s inner ring road within the stadtpilot project. In 2011 IEEE Intelligent Vehicles Symposium (IV), pages 169–174. IEEE, 2011.
<a href="#fnref:29" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:30" class="footnote-text"><span>C. Urmson, J. Anhalt, D. Bagnell, C. Baker, R. Bittner, M. Clark, J. Dolan, D. Duggins, T. Galatali, C. Geyer, et al. Autonomous driving in urban environments: Boss and the urban challenge. Journal of Field Robotics, 25(8):425–466, 2008.
<a href="#fnref:30" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:31" class="footnote-text"><span>A. Schieben, M. Heesen, J. Schindler, J. Kelsch, and F. Flemisch. The theater-system technique: Agile designing and testing of system behavior and interaction, applied to highly automated vehicles. In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications, pages 43–46. ACM, 2009.
<a href="#fnref:31" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:32" class="footnote-text"><span>H. Elrofai, D. Worm, and O. O. den Camp. Scenario identification for validation of automated driving functions. In Advanced Microsystems for Automotive Applications 2016, pages 153–163. Springer, 2016.
<a href="#fnref:32" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:33" class="footnote-text"><span>J. Wang, C. Zhang, Y. Liu, and Q. Zhang. Traffic sensory data classification by quantifying scenario complexity. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1543–1548. IEEE, 2018.
<a href="#fnref:33" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:34" class="footnote-text"><span>C. Zhang, Y. Liu, L. Li, N.-N. Zheng, and F.-Y. Wang. Joint task difficulties estimation and testees ranking for intelligence evaluation. IEEE Transactions on Computational Social Systems, 6(2):221–226, 2019.
<a href="#fnref:34" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:35" class="footnote-text"><span>C. Zhang, Y. Liu, Q. Zhang, and L. Wang. A graded offline evaluation framework for intelligent vehicle’s cognitive ability. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 320–325. IEEE, 2018.
<a href="#fnref:35" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:36" class="footnote-text"><span>M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338, 2010.
<a href="#fnref:36" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:37" class="footnote-text"><span>A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354–3361. IEEE, 2012.
<a href="#fnref:37" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:38" class="footnote-text"><span>Maddern W, Pascoe G, Linegar C, et al. 1 year, 1000 km: The Oxford RobotCar dataset[J]. The International Journal of Robotics Research, 2017, 36(1): 3-15.
<a href="#fnref:38" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:39" class="footnote-text"><span>M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016.
<a href="#fnref:39" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:40" class="footnote-text"><span>X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin, and R. Yang. The apolloscape dataset for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 954–960, 2018.
<a href="#fnref:40" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:41" class="footnote-text"><span>F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell. Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2018.
<a href="#fnref:41" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:42" class="footnote-text"><span>J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281–305, 2012.
<a href="#fnref:42" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:43" class="footnote-text"><span>E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-scale evolution of image classifiers. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2902–2911. JMLR. org, 2017.
<a href="#fnref:43" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:44" class="footnote-text"><span>Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized Evolution for Image Classifier Architecture Search. In arXiv:1802.01548, February 2018.
<a href="#fnref:44" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:45" class="footnote-text"><span>I. Bello, B. Zoph, V. Vasudevan, and Q. V. Le. Neural optimizer search with reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 459–468. JMLR.org, 2017.
<a href="#fnref:45" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:46" class="footnote-text"><span>E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
<a href="#fnref:46" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:47" class="footnote-text"><span>B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
<a href="#fnref:47" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:48" class="footnote-text"><span>B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697–8710, 2018.
<a href="#fnref:48" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:49" class="footnote-text"><span>N. Kalra and S. M. Paddock, “Driving to safety: How many miles of driving would it take to demonstrate autonomous vehicle reliability?” Transp. Res. A, Policy Pract., vol. 94, pp. 182–193, Dec. 2016.
<a href="#fnref:49" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:50" class="footnote-text"><span>Zhou B, Lapedriza A, Khosla A, et al. Places: A 10 million image database for scene recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 40(6): 1452-1464.
<a href="#fnref:50" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:51" class="footnote-text"><span>Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.
<a href="#fnref:51" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:52" class="footnote-text"><span>Zhong Z, Zheng L, Cao D, et al. Re-ranking person re-identification with k-reciprocal encoding[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 1318-1327.
<a href="#fnref:52" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:53" class="footnote-text"><span>R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.
<a href="#fnref:53" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:54" class="footnote-text"><span>S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
<a href="#fnref:54" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:55" class="footnote-text"><span>Elsken T, Metzen J H, Hutter F. Neural architecture search: A survey[J]. J. Mach. Learn. Res., 2019, 20(55): 1-21.
<a href="#fnref:55" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/04/22/%E9%9D%A2%E8%AF%95%E5%A4%8D%E7%9B%98%EF%BC%9A%E8%9A%82%E8%9A%81cro%E7%BA%BF%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8%E9%83%A8%E9%97%A8%E9%9D%A2%E7%BB%8F2+1/">
                        <span class="hidden-mobile">蚂蚁大安全java开发实习面经（2+1）</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments">
                
                  
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    Fluid.utils.waitElementVisible('vcomments', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "zTPGNcnnYmT218Av5iBL8nGh-gzGzoHsz",
          app_key: "Nk68rUDzBxauuv3b7ro1QdtD",
          placeholder: "吐槽",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: false,
          recordIP: false,
          serverURLs: "https://ztpgncnn.lc-cn-n1-shared.com",
        });
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the
    <a target="_blank" href="https://valine.js.org" rel="nofollow noopener noopener">comments powered by Valine.</a>
  </noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      $('#local-search-input').on('click', function() {
        searchFunc(path, 'local-search-input', 'local-search-result');
      });
      $('#modalSearch').on('shown.bs.modal', function() {
        $('#local-search-input').focus();
      });
    })()
  </script>












  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>



</body>
</html>
